{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BlobGANDemo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fractalsproject/blobganplay/blob/main/BlobGANDemo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BlobGAN Playground** \n",
        "\n",
        "This is an official Colab notebook for running and interacting with a pre-trained [BlobGAN](https://www.dave.ml/blobgan) checkpoint. We can use this to generate and animate scenes and even to edit real-world images, too. For example, you can move furniture around to remodel a bedroom and enlarge windows to let in some more natural light.\n",
        "\n",
        "**Make sure you've selected a GPU accelerator (Runtime > Change runtime type > Hardware accelerator > GPU).**\n",
        "\n",
        "Notebook structure borrowed from Bill Peebles' [GANgealing](https://www.wpeebles.com/gangealing). Thanks Bill!\n",
        "\n",
        "Run cells by pressing the ▶️ play icon on the left-hand side. Start with the setup cell below!"
      ],
      "metadata": {
        "id": "WJ8vUUYnI4JL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "Run this cell to get things started. It may take a few minutes to install all the prerequisites, but you only have to run it once. You may see errors pop up with text like \"Your session crashed for an unknown reason\", since we restart the environment to update libraries. Please ignore them and move onto the next cell when execution is complete."
      ],
      "metadata": {
        "id": "cizgRSB_TGS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "!git clone --quiet https://github.com/dave-epstein/blobgan.git\n",
        "import blobgan, os, sys, time\n",
        "!pip -q install moviepy==1.0.3 tqdm==4.64.0 hydra-core==1.1.2 omegaconf==2.1.2 demoji==1.1.0 mpl_interactions[jupyter]==0.21.0 clean-fid==0.1.23 wandb==0.12.16 lpips==0.1.4 einops==0.4.1 inputimeout==1.0.4 pytorch-lightning==1.6.3 torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "# !pip -q install moviepy==1.0.3 tqdm omegaconf demoji mpl_interactions[jupyter] clean-fid wandb lpips einops inputimeout pytorch-lightning torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "!wget -q --show-progress https://github.com/ninja-build/ninja/releases/download/v1.10.2/ninja-linux.zip\n",
        "!sudo unzip -q ninja-linux.zip -d /usr/local/bin/\n",
        "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force\n",
        "from google.colab import output\n",
        "output.clear()\n",
        "print('\\033[92m' 'Done configuring! Move on to the next cell.', flush=True)\n",
        "time.sleep(0.2)\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "sFXfEU7DJZX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Select model\n",
        "\n",
        "You can choose between many different pre-trained models trained on 🛏️ bedrooms, 🍳 kitchens, 🛋️ living rooms, 🍽️ dining rooms, and 🤝🏻 conference rooms.\n",
        "\n",
        "Downloading and configuring models may take a few minutes. Don't worry about any warnings that get printed out.\n",
        "\n",
        "To try out a new model, just ▶️🔄 re-run this cell after changing your selection."
      ],
      "metadata": {
        "id": "0QuVIyVplOKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up imports and path\n",
        "import os, sys\n",
        "PYTHONPATH = '/env/python:/content/blobgan'\n",
        "if os.environ['PYTHONPATH'] != PYTHONPATH:\n",
        "  %matplotlib ipympl\n",
        "  import matplotlib.pyplot as plt\n",
        "  import mpl_interactions.ipyplot as iplt\n",
        "  import numpy as np\n",
        "  import demoji\n",
        "  import ipywidgets as widgets\n",
        "  import torch\n",
        "  from google.colab import output, files\n",
        "  from IPython.display import HTML\n",
        "  from PIL import ImageDraw, Image, ImageFont\n",
        "  from tqdm.notebook import tqdm\n",
        "  output.enable_custom_widget_manager()\n",
        "  sys.path.append('./src')\n",
        "  os.chdir('blobgan')\n",
        "  os.environ['PYTHONPATH'] = PYTHONPATH\n",
        "  import torchvision.transforms.functional as FF\n",
        "  from utils.colab import *\n",
        "  from moviepy.editor import *\n",
        "  from models import BlobGAN\n",
        "  from torchvision.datasets.utils import download_url\n",
        "  from utils.io import BED_CONF_COLORS, KLD_COLORS\n",
        "\n",
        "# Util functions\n",
        "def viz_score_fn(score):\n",
        "    score = score.clone()\n",
        "    score[..., 1:].mul_(2).clamp_(max=1)\n",
        "    return score\n",
        "def norm_img(img):\n",
        "  return img.add(1).div(2).clamp(min=0, max=1)\n",
        "def for_canvas(img):\n",
        "  return img[0].round().permute(1,2,0).clamp(min=0,max=255).cpu().numpy().astype(np.uint8)\n",
        "def draw_labels(img, layout, T):\n",
        "  font = ImageFont.truetype('/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf', 20)\n",
        "  img = Image.fromarray(img)\n",
        "  draw = ImageDraw.Draw(img)\n",
        "  mask = layout['sizes'][0, 1:] > T\n",
        "  idmap = torch.arange(len(mask))[mask]\n",
        "  blob = {k: layout[k][0][mask].mul(255).tolist() for k in ('xs','ys')}\n",
        "  for i, (x,y) in enumerate(zip(blob['xs'], blob['ys'])):\n",
        "    I = idmap[i]\n",
        "    _,h = draw.textsize(str(I), font=font)\n",
        "    w=h\n",
        "    color = tuple(COLORS[I+1].mul(255).round().int().tolist())\n",
        "    draw.text((x - w/2,y - h/2), f'{I}', fill=color, stroke_width=1, font=font, stroke_fill=(0,0,0))\n",
        "  return FF.to_tensor(img).permute(1,2,0), img\n",
        "def clone_layout(l):\n",
        "  return {k: (v if isinstance(v, bool) else (v.clone() if torch.is_tensor(v) else {kk: vv.clone().repeat_interleave(1, 0) for kk,vv in v.items()})) for k,v in l.items() if v is not None}\n",
        "\n",
        "# Configure choice\n",
        "#@markdown Select your model.\n",
        "model = '\\uD83D\\uDECF\\uFE0F bedrooms'  #@param ['🛏️ bedrooms', '🍳 kitchens, 🛋️ living rooms, 🍽️ dining rooms', '🤝🏻 conference rooms']\n",
        "model_name = model\n",
        "\n",
        "# Download model and set up defaults\n",
        "ckpt = download_model(model_name)\n",
        "model = BlobGAN.load_from_checkpoint(ckpt, strict=False).to('cuda')\n",
        "model.mean_latent = download_mean_latent(model_name).to('cuda')\n",
        "model.cherry_picked = download_cherrypicked(model_name).to('cuda')\n",
        "COLORS = KLD_COLORS if 'kitchens' in model_name else BED_CONF_COLORS\n",
        "noise = [torch.randn((1, 1, 16 * 2**((i + 1)//2), 16 * 2**((i + 1)//2))).to('cuda') for i in range(model.generator_ema.num_layers)]\n",
        "render_kwargs = {\n",
        "    'no_jitter': True,\n",
        "    'ret_layout': True,\n",
        "    'viz': True,\n",
        "    'ema': True,\n",
        "    'viz_colors': COLORS,\n",
        "    'norm_img': True,\n",
        "    'viz_score_fn': viz_score_fn,\n",
        "    'noise': noise\n",
        "}\n",
        "size_threshold = -3 if 'bedroom' in model_name else -2\n",
        "z = None\n",
        "output.clear()\n",
        "print('\\033[92m' 'Done loading and configuring model!',flush=True)"
      ],
      "metadata": {
        "id": "z5w45OLChwZy",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Editing scenes\n",
        "\n",
        "Choose your image, whether ⚙️ generated by our model or (coming soon!) 🌎 from the real world, and get to editing!\n",
        "\n",
        "**Click and drag anywhere on the image or blob map to move the corresponding object.**\n",
        "\n",
        "You can also double-click to make objects bigger and right-click to make them smaller, or use the buttons that will appear below after running the cell ▶️.\n",
        "\n",
        "To try out a new image, just ▶️🔄 re-run the cell after you've made your new selection."
      ],
      "metadata": {
        "id": "X4Ux50wEKFdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "#@markdown First, choose an image source. For each model, we've pre-selected 10 cherry picked examples that we think are interesting and fun.\n",
        "image = '\\uD83C\\uDF52 cherry picked 4\\uFE0F\\u20E3'  #@param ['🍒 cherry picked 1️⃣', '🍒 cherry picked 2️⃣', '🍒 cherry picked 3️⃣', '🍒 cherry picked 4️⃣', '🍒 cherry picked 5️⃣', '🍒 cherry picked 6️⃣', '🍒 cherry picked 7️⃣', '🍒 cherry picked 8️⃣', '🍒 cherry picked 9️⃣', '🍒 cherry picked 🔟', '🎲 random', '🎨 van gogh (coming soon!)', '📸 your uploaded image (coming soon!)']\n",
        "#@markdown If you selected `🎲 random`, choose whether to generate a new random image. Check this box to change settings for a random image you already like!\n",
        "keep_random_image = False #@param {type:\"boolean\"}\n",
        "#@markdown If you chose a generated image (`🍒 cherry picked` or `🎲 random`), select the truncation level. We recommend values between 0.3 and 0.5 (on the higher end for non-bedroom data).\n",
        "truncate = 0.4 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "#@markdown We also automatically record interactions with the scene and can render them into a 🎥 video. Set the FPS here.\n",
        "record_fps = 60 #@param {type:\"slider\", min:10, max:120, step:10}\n",
        "#@markdown **Tip:** you can also just click a blob and modify it using the buttons below the image, or using your keyboard ( -/+: shrink/enlarge, arrows: move around, R: reset).\n",
        "\n",
        "#@markdown When you're all done configuring, run this cell!\n",
        "\n",
        "# Generate image and blobs\n",
        "if 'random' in image:\n",
        "  if z is None or not keep_random_image:\n",
        "    z = torch.randn((1, 512)).to('cuda')\n",
        "elif 'cherry' in image:\n",
        "  cherryidx = [int(v.split()[-1]) - 1 for v in demoji.findall(image.encode('utf-16', 'surrogatepass').decode('utf-16')).values() if 'keycap' in v][0]\n",
        "  z = model.cherry_picked[cherryidx:cherryidx+1]\n",
        "else: raise NotImplementedError('Not available yet!')\n",
        "layout, orig_img = model.gen(z=z, truncate=truncate, **render_kwargs)\n",
        "orig_blobs = for_canvas(layout['feature_img'].mul(255))\n",
        "labeled_blobs, labeled_blobs_img = draw_labels(orig_blobs, layout, size_threshold)\n",
        "\n",
        "blobs = DraggableBlobMap(locals())"
      ],
      "metadata": {
        "id": "1aZyivS0FcMG",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coming soon: upload your own image for editing!\n",
        "\n",
        "Here's where you will be able to select your own image to try and edit by inverting it into our model. Note that this feature is experimental and will work better on some images than others. \n",
        "\n",
        "<!-- You'll begin by **running the cell below and uploading an image**. The cell won't stop running until you upload an image file or cancel. -->"
      ],
      "metadata": {
        "id": "H9j_-dpiH9fE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "from google.colab import files\n",
        "from pathlib import Path\n",
        "uploaded = files.upload()\n",
        "## TODO"
      ],
      "metadata": {
        "id": "u9HIPYoeIa3u",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}